{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reitezuz/notebooks-for-NES2-2024/blob/main/lecture_07/cnn_mnist_generalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfX7M9hjkWsC"
      },
      "source": [
        "# Convolutional Neural Network - classifying digits from the MNIST dataset\n",
        "\n",
        "Inspired by: https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter08_intro-to-dl-for-computer-vision.ipynb\n",
        "\n",
        "MNIST dataset is a dataset of handwritten digits. It contains a training set of 60000 greyscale 28x28 images and a testing set of 10000 images of digits written by different people.\n",
        "\n",
        "https://yann.lecun.com/exdb/mnist/\n",
        "https://en.wikipedia.org/wiki/MNIST_database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrC5aInejvQx"
      },
      "source": [
        "## Load and preprocess the data\n",
        "- when training CNNs, we don't need the input patterns to be flatten vectors, 3D tensors are OK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cwVnJ6fdrDgr",
        "outputId": "f56ab8e8-ecb1-4b19-ec24-03e320b7332d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "(60000, 28, 28, 1) (60000,)\n",
            "(60000, 10)\n",
            "[5 0 4]\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "\n",
        "# Load the MNIST dataset\n",
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# 1. Reshape and normalize the data:\n",
        "# reshape the data into 4D tensors (54000x28x28x1) and normalize them to the range [0, 1]\n",
        "x_train = train_images.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
        "x_test = test_images.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
        "y_train = train_labels\n",
        "y_test = test_labels\n",
        "print(x_train.shape, y_train.shape)\n",
        "\n",
        "# 2. Arbitrary: one-hot encode the labels:\n",
        "# For example, the label 3 would become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0].\n",
        "y_train_categorical = keras.utils.to_categorical(train_labels, num_classes=10)\n",
        "y_test_categorical  = keras.utils.to_categorical(train_labels, num_classes=10)\n",
        "\n",
        "print(y_train_categorical.shape)\n",
        "print(y_train[:3])\n",
        "print(y_train_categorical[:3])\n",
        "\n",
        "# 3. Split the training data into training and validation sets\n",
        "# The validation set is used to monitor the performance of the model during training and prevent overfitting.\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_images.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
        "x_test = test_images.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
        "\n",
        "\n",
        "y_train = keras.utils.to_categorical(train_labels, num_classes=10)\n",
        "y_test  = keras.utils.to_categorical(train_labels, num_classes=10)\n",
        "\n",
        "# 3. Split the training data into training and validation sets\n",
        "# The validation set is used to monitor the performance of the model during training and prevent overfitting.\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)\n"
      ],
      "metadata": {
        "id": "GqxdOUz7qd5P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiItbng2zUM3"
      },
      "source": [
        "## Define and train the model\n",
        "\n",
        "### Simple CNN model for multiclass classification:\n",
        "- Start with a keras.Input layer specifying the input shape.\n",
        "- Add multiple blocks of Conv2D (with 'relu' activation) followed by MaxPooling2D.\n",
        "- Then add a Flatten layer to convert 2D feature maps to 1D.\n",
        "- Include one or more Dense layers with 'relu' (or 'tanh') activations for feature extraction.\n",
        "- Use 'softmax' activation function in the output layer for multiclass classification.\n",
        "\n",
        "- Loss function and metrics based on label encoding:\n",
        "    - If labels are one-hot vectors, use `CategoricalCrossentropy` as the loss function and `CategoricalAccuracy` as the metric.\n",
        "    - If labels are provided as integers, use `SparseCategoricalCrossentropy` as the loss function and `SparseCategoricalAccuracy` as the metric.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TCatYpJWjvQ1"
      },
      "outputs": [],
      "source": [
        "# Data frame for results\n",
        "import pandas as pd\n",
        "\n",
        "columns = [\"Model Name\", \"Details\", \"Test Accuracy\", \"Test Loss\"]\n",
        "results_df = pd.DataFrame(columns=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "qGV8yvJgzSU3",
        "outputId": "39ff952a-5020-4e98-9535-2aeb482e769a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m320\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m204,928\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "\u001b[1m325/844\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 83ms/step - accuracy: 0.8251 - loss: 1.0236"
          ]
        }
      ],
      "source": [
        "###############################################\n",
        "# Define the log directory for TensorBoard\n",
        "import os\n",
        "import datetime\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "model_datetime_name = \"mnist_cnn_\"+  datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = \"./logs/fit_mnist/\" + model_datetime_name\n",
        "# print(os.getcwd())\n",
        "\n",
        "# Clear any logs from previous runs\n",
        "# !rm -rf ./logs/\n",
        "\n",
        "# Ensure the directory exists\n",
        "if not os.path.exists(os.path.dirname(log_dir)):\n",
        "    os.makedirs(os.path.dirname(log_dir))\n",
        "\n",
        "###############################################\n",
        "# Initialize Tensorboard callback\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(\n",
        "    log_dir=log_dir,\n",
        "    histogram_freq=1,  # Visualize histograms of layer weights\n",
        "    write_graph=True,  # Log the graph to visualize the model structure\n",
        "    write_images=True  # Optionally, save images of weights and activation histograms\n",
        "    # update_freq='batch'  # Log metrics after every batch\n",
        "    # write_steps_per_second=True  # Log steps per second during training\n",
        ")\n",
        "################################################\n",
        "\n",
        "\n",
        "# define the model architecture\n",
        "model = keras.Sequential([\n",
        "    keras.Input(shape=(28, 28, 1)),\n",
        "    keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    keras.layers.Flatten(),\n",
        "\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    #keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Set model parameters\n",
        "model.compile(optimizer= keras.optimizers.Adam(),   # SGD, Adam, RMSProp\n",
        "              loss= keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "              metrics= [keras.metrics.CategoricalAccuracy(\"accuracy\")])\n",
        "\n",
        "num_epochs = 8\n",
        "batch_size = 64\n",
        "\n",
        "# Early stopping:\n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    epochs=num_epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[tensorboard_callback, early_stopping])\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc, '\\nTest loss:', test_loss)\n",
        "\n",
        "# Save the model\n",
        "model_dir = \"./models/\"\n",
        "# Ensure the directory exists\n",
        "if not os.path.exists(os.path.dirname(model_dir)):\n",
        "    os.makedirs(os.path.dirname(model_dir))\n",
        "model_name = model_dir + model_datetime_name + \".keras\"\n",
        "model.save(model_name)\n",
        "\n",
        "model_details = \"32 64 - 128 label smoothing\"\n",
        "# Add results to the dataframe:\n",
        "new_entry = {\n",
        "    \"Model Name\" : model_datetime_name,\n",
        "    \"Details\" : model_details,\n",
        "    \"Test Accuracy\" : test_acc,\n",
        "    \"Test Loss\" : test_loss,\n",
        "}\n",
        "results_df = pd.concat([results_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
        "\n",
        "# View and and save the dataframe:\n",
        "print(\"Results:\")\n",
        "print(results_df)\n",
        "results_df.to_csv(model_dir + \"mnist_cnn_results.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU8cUnQS-oVY"
      },
      "outputs": [],
      "source": [
        "# plot the training progress:\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history_dict['accuracy'])\n",
        "plt.plot(history_dict['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history_dict['loss'])\n",
        "plt.plot(history_dict['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "train_loss, train_acc = model.evaluate(x_train, y_train)\n",
        "print('Training accuracy:', train_acc, '\\nTrain loss:', train_loss)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_acc = model.evaluate(x_val, y_val)\n",
        "print('Validation accuracy:', val_acc, '\\nVal loss:', val_loss)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc, '\\nTest loss:', test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9oYaO_138aa"
      },
      "source": [
        "## Evaluate the model and make predictions on new data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNHkFYxs_K9T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Get predicted probabilities for the test set\n",
        "y_pred_probs = model.predict(x_test)\n",
        "print(y_pred_probs[0])\n",
        "\n",
        "# Get the predicted class for each sample\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "print(\"Predicted labels:\", y_pred[:10])\n",
        "print(\"True labels:\", y_test[:10])\n",
        "\n",
        "# Misclassified indices:\n",
        "misclassified_indices = np.where(y_pred != y_test)[0]\n",
        "num_misclassified = len(misclassified_indices)\n",
        "print(\"Number of misclassified images:\", num_misclassified,\n",
        "      \"out of\", len(y_test), \", accuracy\", test_acc)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Plot some misclassified images\n",
        "num_images_to_plot = 5\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(min(num_images_to_plot, len(misclassified_indices))):\n",
        "    index = misclassified_indices[i]\n",
        "    plt.subplot(1, num_images_to_plot, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(test_images[index], cmap=plt.cm.binary)\n",
        "    plt.xlabel(f\"Pred: {y_pred[index]}, True: {y_test[index]}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6XQlpZb__0M"
      },
      "outputs": [],
      "source": [
        "# Plot some misclassified images from a given target (or predicted) class\n",
        "target_class = 3\n",
        "misclassified_indices_class = np.where((y_pred != y_test) & (y_test == target_class))[0]\n",
        "#misclassified_indices_class = np.where((y_pred != y_test) & (y_pred == target_class))[0]\n",
        "\n",
        "\n",
        "\n",
        "# Display the first 25 misclassified images for the target class\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(min(25, len(misclassified_indices_class))):\n",
        "    index = misclassified_indices_class[i]\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(test_images[index], cmap=plt.cm.binary)\n",
        "    plt.xlabel(f\"True:{y_test[index]}, Pred:{y_pred[index]}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn-GJqLSQ-dO"
      },
      "source": [
        "# Exercises\n",
        "1. **Compare the results with MLP** (see multiclass_classification_mnist.ipynb). In both cases, use the same optimizer (e.g., Adam) and appropriate number of epochs.  \n",
        "2. **Experiment with the architecture**. For example, use only one convolutional block with additional dense layers. Experiment with kernel sizes and the number of filters.\n",
        "\n",
        "# More exercises\n",
        "1. Experiment with the number of epochs and batch size.\n",
        "2. Change the optimizer of the model.\n",
        "3. Try standardization of the data instead of simple normalization to [0, 1].\n",
        "4. Use one-hot encoding of the output data with `CategoricalCrossentropy` as the loss function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb5_3CDbjvQ7"
      },
      "outputs": [],
      "source": [
        "###############################################\n",
        "# Load TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Start TensorBoard before training begins\n",
        "%tensorboard --logdir logs/fit_mnist --reload_interval=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KctSjuK_jvQ9"
      },
      "outputs": [],
      "source": [
        "# Early stopping:\n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
        "\n",
        "# ... callbacks=[tensorboard_callback, early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropout\n",
        "# keras.layers.Dropout(0.5), # Optional dropout after each dense layer"
      ],
      "metadata": {
        "id": "3nEeb7fipCW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch normalization\n",
        "model = keras.Sequential([\n",
        "    keras.Input(shape=(28, 28, 1)),\n",
        "\n",
        "    keras.layers.Conv2D(32, kernel_size=(3, 3)),\n",
        "    keras.layers.BatchNormalization(),  # Add batch normalization\n",
        "    keras.layers.Activation('relu'),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    keras.layers.Conv2D(64, kernel_size=(3, 3)),\n",
        "    keras.layers.BatchNormalization(),  # Add batch normalization\n",
        "    keras.layers.Activation('relu'),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    keras.layers.Flatten(),\n",
        "\n",
        "    keras.layers.Dense(128),\n",
        "    keras.layers.BatchNormalization(),  # Add batch normalization\n",
        "    keras.layers.Activation('relu'),\n",
        "\n",
        "    keras.layers.Dense(10, activation='softmax')  # Output layer\n",
        "])"
      ],
      "metadata": {
        "id": "PZUhV3g-oqOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label smoothing ... works only with Categorical crossentropy (not with Sparse version)\n",
        "model.compile(optimizer= keras.optimizers.Adam(),   # SGD, Adam, RMSProp\n",
        "              loss= keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "              metrics= [keras.metrics.CategoricalAccuracy(\"accuracy\")])"
      ],
      "metadata": {
        "id": "_cpEW-GboTBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensembling:"
      ],
      "metadata": {
        "id": "JgO3uULXomgL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}