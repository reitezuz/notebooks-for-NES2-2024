{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO533koKyIH0qA10DOXnsYE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reitezuz/notebooks-for-NES2-2024/blob/main/lecture_12/TextVectorization_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TextVectorization layer\n",
        "https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/"
      ],
      "metadata": {
        "id": "YkMIXyL-RP6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",  # \"int\", \"multi_hot\", \"count\" or \"tf_idf\"\n",
        "   )\n",
        "'''\n",
        "keras.layers.TextVectorization(\n",
        "    max_tokens=None,\n",
        "    standardize=\"lower_and_strip_punctuation\", # default standardization\n",
        "    split=\"whitespace\",                        # default split\n",
        "    ngrams=None,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=None,\n",
        "    pad_to_max_tokens=False,\n",
        "    vocabulary=None,\n",
        "    idf_weights=None,\n",
        "    sparse=False,\n",
        "    ragged=False,\n",
        "    encoding=\"utf-8\",\n",
        "    name=None,\n",
        "    **kwargs\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "2VlIZpRkwEoL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5d265a3e-4cdf-4f00-dd6a-c4aea6e632b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nkeras.layers.TextVectorization(\\n    max_tokens=None,\\n    standardize=\"lower_and_strip_punctuation\",\\n    split=\"whitespace\",\\n    ngrams=None,\\n    output_mode=\"int\",\\n    output_sequence_length=None,\\n    pad_to_max_tokens=False,\\n    vocabulary=None,\\n    idf_weights=None,\\n    sparse=False,\\n    ragged=False,\\n    encoding=\"utf-8\",\\n    name=None,\\n    **kwargs\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IMvZtIYivNSk"
      },
      "outputs": [],
      "source": [
        "# Text vectorization with custom standardization and custom split\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from keras.layers import TextVectorization\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "    lowercase_string = tf.strings.lower(string_tensor)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "    return tf.strings.split(string_tensor)\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the vocabulary:"
      ],
      "metadata": {
        "id": "ararNlBxTgil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "id": "6skq5HIPvgYr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary:"
      ],
      "metadata": {
        "id": "Ra28HqkrS5wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.get_vocabulary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn1llI1ivnen",
        "outputId": "d8c8292a-379b-4e86-bf26-18b5c4bdd8ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorize text:"
      ],
      "metadata": {
        "id": "77vV7KEjTAcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and you rewrite again.\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb_kMZXFv1ni",
        "outputId": "1eb7734e-679e-471d-c4b1-254634983bbc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WFgJLI322RVv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}